{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser \n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5q/yq3hk8g1793ckqmn2n3xpr6c0000gn/T/ipykernel_16694/2170368664.py:1: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"data/cleaned_data2.csv\")\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/cleaned_data2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data[\"tokenized\"] = data[\"cleaned_text\"].apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom defined list\n",
    "english = [\n",
    "    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \n",
    "    \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"cannot\", \"could\", \"did\", \n",
    "    \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \n",
    "    \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \n",
    "    \"its\", \"itself\", \"let\", \"me\", \"more\", \"most\", \"must\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \"off\", \"on\", \n",
    "    \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"some\", \"such\", \n",
    "    \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \n",
    "    \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \n",
    "    \"while\", \"who\", \"whom\", \"why\", \"with\", \"would\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"will\", \"ll\", \n",
    "    \"re\", \"ve\", \"d\", \"s\", \"m\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \n",
    "    \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"many\", \"us\", \"ok\", \"hows\", \"ive\", \"ill\", \"im\", \"cant\", \"topics\", \"topic\",\n",
    "    \"discuss\", \"thoughts\", \"yo\", \"thats\", \"whats\", \"lets\", \"nothing\", \"oh\", \"omg\", \n",
    "         \"things\", \"stuff\", \"yall\", \"haha\", \"yes\", \"no\", \"wo\", \"like\", 'good', \n",
    "         'work', 'got', 'going', 'dont', 'really', 'want', 'make', 'think', \n",
    "         'know', 'feel', 'people', 'life', \"getting\", \"lot\" \"great\", \"i\", \"me\", \n",
    "         \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \n",
    "        \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \n",
    "        \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \n",
    "        \"they\", \"them\", \"their\", \"theirs\",\"themselves\", \"what\", \"which\", \"who\", \n",
    "        \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \n",
    "        \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \n",
    "        \"does\", \"did\", \"doing\", \"will\", \"would\", \"should\", \"can\", \"could\", \"may\",\n",
    "        \"might\", \"must\", \"shall\", \"ought\", \"about\", \"above\", \"across\", \"after\", \n",
    "        \"against\", \"along\", \"amid\", \"among\", \"around\", \"as\", \"at\", \"before\", \"behind\",\n",
    "        \"below\", \"beneath\", \"beside\", \"between\", \"beyond\", \"but\", \"by\", \n",
    "        \"concerning\", \"considering\", \"despite\", \"down\", \"during\", \"except\", \"for\",\n",
    "        \"from\", \"in\", \"inside\", \"into\", \"like\", \"near\", \"next\", \"notwithstanding\",\n",
    "        \"of\", \"off\", \"on\", \"onto\", \"opposite\", \"out\", \"outside\", \"over\", \"past\",\n",
    "        \"regarding\", \"round\", \"since\", \"than\", \"through\", \"throughout\", \"till\", \n",
    "        \"to\", \"toward\", \"towards\", \"under\", \"underneath\", \"unlike\", \"until\", \"up\",\n",
    "        \"upon\", \"versus\", \"via\", \"with\", \"within\", \"without\", \"cant\", \"cannot\", \n",
    "        \"couldve\", \"couldnt\", \"didnt\", \"doesnt\", \"dont\", \"hadnt\", \"hasnt\", \n",
    "        \"havent\", \"hed\", \"hell\", \"hes\", \"howd\", \"howll\", \"hows\", \"id\", \"ill\", \n",
    "        \"im\", \"ive\", \"isnt\", \"itd\", \"itll\", \"its\", \"lets\", \"mightve\", \"mustve\", \n",
    "        \"mustnt\", \"shant\", \"shed\", \"shell\", \"shes\", \"shouldve\", \"shouldnt\", \n",
    "        \"thatll\", \"thats\", \"thered\", \"therell\", \"therere\", \"theres\", \"theyd\", \n",
    "        \"theyll\", \"theyre\", \"theyve\", \"wed\", \"well\", \"were\", \"weve\", \"werent\", \n",
    "        \"whatd\", \"whatll\", \"whatre\", \"whats\", \"whatve\", \"whend\", \"whenll\", \n",
    "        \"whens\", \"whered\", \"wherell\", \"wheres\", \"whichd\", \"whichll\", \"whichre\", \n",
    "        \"whichs\", \"whod\", \"wholl\", \"whore\", \"whos\", \"whove\", \"whyd\", \"whyll\", \n",
    "        \"whys\", \"wont\", \"wouldve\", \"wouldnt\", \"youd\", \"youll\", \"youre\", \"youve\",\n",
    "        \"f\", \"m\", \"because\", \"go\", \"lot\", \"get\", \"still\", \"way\", \"something\", \"much\",\n",
    "        \"thing\", \"someone\", \"person\", \"anything\", \"goes\", \"ok\", \"so\", \"just\", \"mostly\", \n",
    "        \"put\", \"also\", \"lots\", \"yet\", \"ha\", \"etc\", \"wasnt\", \"yeah\", \"okay\", \"lol\"]\n",
    "\n",
    "time = [\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \n",
    "        \"sunday\", \"morning\", \"noon\", \"afternoon\", \"evening\", \"night\", \"midnight\",\n",
    "        \"dawn\", \"dusk\", \"week\", \"weekend\", \"weekends\",\"weekly\", \"today\", \n",
    "        \"yesterday\", \"tomorrow\", \"yesterdays\", \"todays\", \"mondays\", \"tuesdays\",\n",
    "        \"wednesdays\", \"thursdays\", \"fridays\", \"saturdays\", \"sundays\", \"day\",\n",
    "        \"everyday\", \"daily\", \"workday\", 'time', 'month', 'year', 'pm', 'am', \"ago\",\n",
    "        \"year\"]\n",
    "\n",
    "reddit = [\"welcome\", \"hi\", \"hello\", \"sub\", \"reddit\", \"thanks\", \"thank\", \"maybe\",\n",
    "          \"wo30\", \"mods\", \"mod\", \"moderators\", \"subreddit\", \"btw\", \"aw\", \"aww\", \n",
    "          \"aww\", \"hey\", \"hello\", \"join\", \"joined\", \"post\", \"op\"]\n",
    "\n",
    "topic_specific = [\"self\", \"improvement\", \"selfimprovement\", \"rselfimprovement\"]\n",
    "\n",
    "stop_words = english + time + reddit + topic_specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords\n",
    "def remove_stopwords_numbers(tokens):\n",
    "    \"\"\"Removes stopwords and tokens that contain any digits\"\"\"\n",
    "    return [token for token in tokens if token not in stop_words and not re.search(r'\\d', token)]\n",
    "\n",
    "# Apply function to column\n",
    "data[\"filtered_tokens\"] = data[\"tokenized\"].apply(remove_stopwords_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"Lemmatizes a list of tokens\"\"\"\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Apply function to column\n",
    "data[\"lemmatized\"] = data[\"filtered_tokens\"].apply(lemmatize_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.h2kinfosys.com/blog/part-of-speech-tagging-chunking-with-nltk/\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Convert POS tag to a format recognized by WordNetLemmatizer\"\"\"\n",
    "    if tag.startswith('J'): #NLTK tags adjectives as JJ (normal), \n",
    "                            #JJR (comparative) and JJS (superlative)\n",
    "        return wordnet.ADJ  #and then it can be mapped to the wordnet database\n",
    "                              #to get the lemma\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"Lemmatizes tokens with POS tagging\"\"\"\n",
    "    tagged_tokens = pos_tag(tokens)  \n",
    "    return [lemmatizer.lemmatize(word, pos) for word, tag in tagged_tokens\n",
    "            if (pos := get_wordnet_pos(tag))]\n",
    "\n",
    "data[\"lemmatized\"] = data[\"filtered_tokens\"].apply(lemmatize_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train bigram model on the lemmatized column\n",
    "bigram = Phrases(data[\"lemmatized\"], min_count=20)\n",
    "bigram_phraser = Phraser(bigram)  # Optimizes since it is a lighter-weight\n",
    "                                   #version of Phrases\n",
    "\n",
    "def add_bigrams(tokens):\n",
    "    \"\"\"Adds bigrams to a list of tokens if they appear frequently enough.\"\"\"\n",
    "    bigram_tokens = bigram_phraser[tokens]\n",
    "    return tokens + [token for token in bigram_tokens if '_' in token]\n",
    "\n",
    "data[\"ngrams\"] = data[\"lemmatized\"].apply(add_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering of too rare of too common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert df column to a list of lists for later steps\n",
    "docs = data[\"ngrams\"].tolist() \n",
    "\n",
    "# Create a dictionary from the tokenized documents\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that appear in fewer than 20 documents or \n",
    "#  more than 80% of documents, and keep the 1500 most frequent\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.8, keep_n=1500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bag of words representation\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set hyperparameters\n",
    "num_topics = 10\n",
    "chunksize = 2000 #to speed up training\n",
    "passes = 10\n",
    "iterations = 100\n",
    "eval_every = None\n",
    "minimum_probability = 0.10  \n",
    "alpha = 0.01 #discuss this\n",
    "eta = 0.01 #discuss this\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "temp = dictionary[0]  \n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha=alpha,\n",
    "    eta=eta,\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every,\n",
    "    random_state = 21,\n",
    "    minimum_probability=minimum_probability\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -2.2494.\n",
      "[([(0.021396976, 'now'),\n",
      "   (0.017239062, 'right'),\n",
      "   (0.01709818, 'even'),\n",
      "   (0.014876145, 'bad'),\n",
      "   (0.01123997, 'hard'),\n",
      "   (0.011220804, 'never'),\n",
      "   (0.01083543, 'well'),\n",
      "   (0.010381427, 'back'),\n",
      "   (0.009575325, 'always'),\n",
      "   (0.008478604, 'best'),\n",
      "   (0.008397271, 'change'),\n",
      "   (0.007956359, 'everything'),\n",
      "   (0.007848885, 'else'),\n",
      "   (0.00762796, 'try'),\n",
      "   (0.00729698, 'first'),\n",
      "   (0.006985285, 'feeling'),\n",
      "   (0.0068452414, 'situation'),\n",
      "   (0.006826662, 'problem'),\n",
      "   (0.006708201, 'everyone'),\n",
      "   (0.006604008, 'sometimes')],\n",
      "  -1.8251632320699467),\n",
      " ([(0.03353481, 'job'),\n",
      "   (0.025605064, 'year'),\n",
      "   (0.022956245, 'now'),\n",
      "   (0.021113206, 'money'),\n",
      "   (0.016339919, 'school'),\n",
      "   (0.013089951, 'old'),\n",
      "   (0.011937608, 'even'),\n",
      "   (0.011111892, 'age'),\n",
      "   (0.011099764, 'never'),\n",
      "   (0.01096348, 'kid'),\n",
      "   (0.010681453, 'parent'),\n",
      "   (0.010096079, 'young'),\n",
      "   (0.009651819, 'back'),\n",
      "   (0.00943679, 'family'),\n",
      "   (0.00936537, 'college'),\n",
      "   (0.008040453, 'high'),\n",
      "   (0.007337541, 'live'),\n",
      "   (0.0072513567, 'career'),\n",
      "   (0.0071568997, 'hard'),\n",
      "   (0.006778317, 'home')],\n",
      "  -2.0228092874563277),\n",
      " ([(0.07661036, 'friend'),\n",
      "   (0.031170918, 'social'),\n",
      "   (0.020717451, 'new'),\n",
      "   (0.019129345, 'group'),\n",
      "   (0.018087745, 'talk'),\n",
      "   (0.01622769, 'hobby'),\n",
      "   (0.0142828375, 'conversation'),\n",
      "   (0.013510574, 'interest'),\n",
      "   (0.011490577, 'alone'),\n",
      "   (0.010328221, 'relationship'),\n",
      "   (0.010105654, 'meet'),\n",
      "   (0.010053971, 'even'),\n",
      "   (0.008617769, 'friendship'),\n",
      "   (0.008504994, 'others'),\n",
      "   (0.00829112, 'try'),\n",
      "   (0.0075848727, 'family'),\n",
      "   (0.007567903, 'fun'),\n",
      "   (0.007273576, 'always'),\n",
      "   (0.0070102382, 'skill'),\n",
      "   (0.006794579, 'best')],\n",
      "  -2.0783164509700027),\n",
      " ([(0.021176804, 'others'),\n",
      "   (0.011154958, 'world'),\n",
      "   (0.011152318, 'change'),\n",
      "   (0.008667634, 'value'),\n",
      "   (0.0082800435, 'positive'),\n",
      "   (0.008148409, 'experience'),\n",
      "   (0.007977199, 'happy'),\n",
      "   (0.0076622036, 'best'),\n",
      "   (0.007488636, 'negative'),\n",
      "   (0.0073889517, 'mind'),\n",
      "   (0.007109674, 'love'),\n",
      "   (0.0070850216, 'human'),\n",
      "   (0.006730827, 'happiness'),\n",
      "   (0.006520322, 'important'),\n",
      "   (0.006214257, 'different'),\n",
      "   (0.006075106, 'true'),\n",
      "   (0.0058440277, 'control'),\n",
      "   (0.0057898387, 'focus'),\n",
      "   (0.0056323498, 'need'),\n",
      "   (0.00552631, 'well')],\n",
      "  -2.155519449707712),\n",
      " ([(0.031097282, 'body'),\n",
      "   (0.021934455, 'gym'),\n",
      "   (0.019777494, 'weight'),\n",
      "   (0.018441208, 'exercise'),\n",
      "   (0.01734846, 'food'),\n",
      "   (0.016364688, 'eat'),\n",
      "   (0.013345398, 'healthy'),\n",
      "   (0.010990063, 'start'),\n",
      "   (0.010730805, 'water'),\n",
      "   (0.010442668, 'diet'),\n",
      "   (0.008513007, 'muscle'),\n",
      "   (0.008466246, 'routine'),\n",
      "   (0.0075145406, 'even'),\n",
      "   (0.007145975, 'meditation'),\n",
      "   (0.0070242654, 'sleep'),\n",
      "   (0.0068817176, 'fat'),\n",
      "   (0.0068577584, 'try'),\n",
      "   (0.0067908494, 'day'),\n",
      "   (0.0067364033, 'well'),\n",
      "   (0.0066346657, 'calorie')],\n",
      "  -2.2742502542704246),\n",
      " ([(0.03068832, 'game'),\n",
      "   (0.02930545, 'porn'),\n",
      "   (0.023511086, 'hour'),\n",
      "   (0.017597673, 'video'),\n",
      "   (0.017418096, 'phone'),\n",
      "   (0.01567965, 'sleep'),\n",
      "   (0.0149197085, 'addiction'),\n",
      "   (0.014724004, 'brain'),\n",
      "   (0.0127377985, 'day'),\n",
      "   (0.012475337, 'watch'),\n",
      "   (0.011747957, 'even'),\n",
      "   (0.011517103, 'quit'),\n",
      "   (0.010687556, 'bad'),\n",
      "   (0.010541315, 'music'),\n",
      "   (0.009295162, 'now'),\n",
      "   (0.0091201, 'play'),\n",
      "   (0.007971693, 'drug'),\n",
      "   (0.007680991, 'back'),\n",
      "   (0.0074850097, 'video_game'),\n",
      "   (0.007355379, 'use')],\n",
      "  -2.332276822788686),\n",
      " ([(0.030403648, 'help'),\n",
      "   (0.030193174, 'mental'),\n",
      "   (0.02856861, 'health'),\n",
      "   (0.028295029, 'therapy'),\n",
      "   (0.023394616, 'issue'),\n",
      "   (0.02082179, 'therapist'),\n",
      "   (0.01735098, 'depression'),\n",
      "   (0.017310068, 'anxiety'),\n",
      "   (0.015530551, 'mental_health'),\n",
      "   (0.013836721, 'problem'),\n",
      "   (0.012023737, 'relationship'),\n",
      "   (0.009316356, 'professional'),\n",
      "   (0.009165383, 'feeling'),\n",
      "   (0.008835387, 'trauma'),\n",
      "   (0.008330826, 'support'),\n",
      "   (0.008024174, 'doctor'),\n",
      "   (0.0076491823, 'family'),\n",
      "   (0.0076437523, 'emotional'),\n",
      "   (0.007572353, 'emotion'),\n",
      "   (0.00728112, 'bad')],\n",
      "  -2.438065981181125),\n",
      " ([(0.05105499, 'goal'),\n",
      "   (0.031565856, 'step'),\n",
      "   (0.026197286, 'medium'),\n",
      "   (0.026120104, 'habit'),\n",
      "   (0.02579723, 'social'),\n",
      "   (0.024588848, 'small'),\n",
      "   (0.021284549, 'social_medium'),\n",
      "   (0.017710043, 'start'),\n",
      "   (0.013743462, 'first'),\n",
      "   (0.013568743, 'change'),\n",
      "   (0.011800043, 'long'),\n",
      "   (0.011140345, 'task'),\n",
      "   (0.010807719, 'term'),\n",
      "   (0.01001759, 'motivation'),\n",
      "   (0.009706813, 'plan'),\n",
      "   (0.009522029, 'new'),\n",
      "   (0.009148198, 'easy'),\n",
      "   (0.0087403385, 'list'),\n",
      "   (0.008715018, 'progress'),\n",
      "   (0.008704823, 'app')],\n",
      "  -2.448382803971239),\n",
      " ([(0.046397634, 'book'),\n",
      "   (0.020121835, 'question'),\n",
      "   (0.014605236, 'read'),\n",
      "   (0.01399438, 'learn'),\n",
      "   (0.013493773, 'skill'),\n",
      "   (0.0121303955, 'help'),\n",
      "   (0.010085251, 'advice'),\n",
      "   (0.00974026, 'great'),\n",
      "   (0.009348384, 'answer'),\n",
      "   (0.009151359, 'ask'),\n",
      "   (0.009090025, 'different'),\n",
      "   (0.008863091, 'art'),\n",
      "   (0.008623047, 'idea'),\n",
      "   (0.0081267385, 'practice'),\n",
      "   (0.0076830788, 'new'),\n",
      "   (0.007653521, 'best'),\n",
      "   (0.0075990255, 'use'),\n",
      "   (0.007337591, 'youtube'),\n",
      "   (0.007211131, 'information'),\n",
      "   (0.0069333734, 'free')],\n",
      "  -2.4565106159126864),\n",
      " ([(0.061817504, 'woman'),\n",
      "   (0.028741064, 'men'),\n",
      "   (0.027164735, 'girl'),\n",
      "   (0.024173845, 'guy'),\n",
      "   (0.021665998, 'sex'),\n",
      "   (0.021290433, 'relationship'),\n",
      "   (0.017888598, 'man'),\n",
      "   (0.013565081, 'attractive'),\n",
      "   (0.011765259, 'even'),\n",
      "   (0.011672024, 'partner'),\n",
      "   (0.011024888, 'ugly'),\n",
      "   (0.01010245, 'look'),\n",
      "   (0.009681431, 'date'),\n",
      "   (0.009169502, 'confidence'),\n",
      "   (0.007322384, 'never'),\n",
      "   (0.0066335136, 'hair'),\n",
      "   (0.0061886217, 'sexual'),\n",
      "   (0.006087588, 'care'),\n",
      "   (0.0060732053, 'body'),\n",
      "   (0.0057877083, 'personality')],\n",
      "  -2.462786853775979)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.500950930647863"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = CoherenceModel(model=model, corpus=corpus, texts=docs, coherence='c_v', dictionary=dictionary)\n",
    "cm.get_coherence()  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
