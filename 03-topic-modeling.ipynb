{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser \n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5q/yq3hk8g1793ckqmn2n3xpr6c0000gn/T/ipykernel_16694/2170368664.py:1: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"data/cleaned_data2.csv\")\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/cleaned_data2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data[\"tokenized\"] = data[\"cleaned_text\"].apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom defined list\n",
    "english = [\n",
    "    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \n",
    "    \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"cannot\", \"could\", \"did\", \n",
    "    \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \n",
    "    \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \n",
    "    \"its\", \"itself\", \"let\", \"me\", \"more\", \"most\", \"must\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \"off\", \"on\", \n",
    "    \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"some\", \"such\", \n",
    "    \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \n",
    "    \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \n",
    "    \"while\", \"who\", \"whom\", \"why\", \"with\", \"would\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"will\", \"ll\", \n",
    "    \"re\", \"ve\", \"d\", \"s\", \"m\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \n",
    "    \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"many\", \"us\", \"ok\", \"hows\", \"ive\", \"ill\", \"im\", \"cant\", \"topics\", \"topic\",\n",
    "    \"discuss\", \"thoughts\", \"yo\", \"thats\", \"whats\", \"lets\", \"nothing\", \"oh\", \"omg\", \n",
    "         \"things\", \"stuff\", \"yall\", \"haha\", \"yes\", \"no\", \"wo\", \"like\", 'good', \n",
    "         'work', 'got', 'going', 'dont', 'really', 'want', 'make', 'think', \n",
    "         'know', 'feel', 'people', 'life', \"getting\", \"lot\" \"great\", \"i\", \"me\", \n",
    "         \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \n",
    "        \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \n",
    "        \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \n",
    "        \"they\", \"them\", \"their\", \"theirs\",\"themselves\", \"what\", \"which\", \"who\", \n",
    "        \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \n",
    "        \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \n",
    "        \"does\", \"did\", \"doing\", \"will\", \"would\", \"should\", \"can\", \"could\", \"may\",\n",
    "        \"might\", \"must\", \"shall\", \"ought\", \"about\", \"above\", \"across\", \"after\", \n",
    "        \"against\", \"along\", \"amid\", \"among\", \"around\", \"as\", \"at\", \"before\", \"behind\",\n",
    "        \"below\", \"beneath\", \"beside\", \"between\", \"beyond\", \"but\", \"by\", \n",
    "        \"concerning\", \"considering\", \"despite\", \"down\", \"during\", \"except\", \"for\",\n",
    "        \"from\", \"in\", \"inside\", \"into\", \"like\", \"near\", \"next\", \"notwithstanding\",\n",
    "        \"of\", \"off\", \"on\", \"onto\", \"opposite\", \"out\", \"outside\", \"over\", \"past\",\n",
    "        \"regarding\", \"round\", \"since\", \"than\", \"through\", \"throughout\", \"till\", \n",
    "        \"to\", \"toward\", \"towards\", \"under\", \"underneath\", \"unlike\", \"until\", \"up\",\n",
    "        \"upon\", \"versus\", \"via\", \"with\", \"within\", \"without\", \"cant\", \"cannot\", \n",
    "        \"couldve\", \"couldnt\", \"didnt\", \"doesnt\", \"dont\", \"hadnt\", \"hasnt\", \n",
    "        \"havent\", \"hed\", \"hell\", \"hes\", \"howd\", \"howll\", \"hows\", \"id\", \"ill\", \n",
    "        \"im\", \"ive\", \"isnt\", \"itd\", \"itll\", \"its\", \"lets\", \"mightve\", \"mustve\", \n",
    "        \"mustnt\", \"shant\", \"shed\", \"shell\", \"shes\", \"shouldve\", \"shouldnt\", \n",
    "        \"thatll\", \"thats\", \"thered\", \"therell\", \"therere\", \"theres\", \"theyd\", \n",
    "        \"theyll\", \"theyre\", \"theyve\", \"wed\", \"well\", \"were\", \"weve\", \"werent\", \n",
    "        \"whatd\", \"whatll\", \"whatre\", \"whats\", \"whatve\", \"whend\", \"whenll\", \n",
    "        \"whens\", \"whered\", \"wherell\", \"wheres\", \"whichd\", \"whichll\", \"whichre\", \n",
    "        \"whichs\", \"whod\", \"wholl\", \"whore\", \"whos\", \"whove\", \"whyd\", \"whyll\", \n",
    "        \"whys\", \"wont\", \"wouldve\", \"wouldnt\", \"youd\", \"youll\", \"youre\", \"youve\",\n",
    "        \"f\", \"m\", \"because\", \"go\", \"lot\", \"get\", \"still\", \"way\", \"something\", \"much\",\n",
    "        \"thing\", \"someone\", \"person\", \"anything\", \"goes\", \"ok\", \"so\", \"just\", \"mostly\", \n",
    "        \"put\", \"also\", \"lots\", \"yet\", \"ha\", \"etc\", \"wasnt\", \"yeah\", \"okay\", \"lol\"]\n",
    "\n",
    "time = [\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \n",
    "        \"sunday\", \"morning\", \"noon\", \"afternoon\", \"evening\", \"night\", \"midnight\",\n",
    "        \"dawn\", \"dusk\", \"week\", \"weekend\", \"weekends\",\"weekly\", \"today\", \n",
    "        \"yesterday\", \"tomorrow\", \"yesterdays\", \"todays\", \"mondays\", \"tuesdays\",\n",
    "        \"wednesdays\", \"thursdays\", \"fridays\", \"saturdays\", \"sundays\", \"day\",\n",
    "        \"everyday\", \"daily\", \"workday\", 'time', 'month', 'year', 'pm', 'am', \"ago\",\n",
    "        \"year\"]\n",
    "\n",
    "reddit = [\"welcome\", \"hi\", \"hello\", \"sub\", \"reddit\", \"thanks\", \"thank\", \"maybe\",\n",
    "          \"wo30\", \"mods\", \"mod\", \"moderators\", \"subreddit\", \"btw\", \"aw\", \"aww\", \n",
    "          \"aww\", \"hey\", \"hello\", \"join\", \"joined\", \"post\", \"op\"]\n",
    "\n",
    "topic_specific = [\"self\", \"improvement\", \"selfimprovement\", \"rselfimprovement\"]\n",
    "\n",
    "stop_words = english + time + reddit + topic_specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords\n",
    "def remove_stopwords_numbers(tokens):\n",
    "    \"\"\"Removes stopwords and tokens that contain any digits\"\"\"\n",
    "    return [token for token in tokens if token not in stop_words and not re.search(r'\\d', token)]\n",
    "\n",
    "# Apply function to column\n",
    "data[\"filtered_tokens\"] = data[\"tokenized\"].apply(remove_stopwords_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"Lemmatizes a list of tokens\"\"\"\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Apply function to column\n",
    "data[\"lemmatized\"] = data[\"filtered_tokens\"].apply(lemmatize_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.h2kinfosys.com/blog/part-of-speech-tagging-chunking-with-nltk/\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Convert POS tag to a format recognized by WordNetLemmatizer\"\"\"\n",
    "    if tag.startswith('J'): #NLTK tags adjectives as JJ (normal), \n",
    "                            #JJR (comparative) and JJS (superlative)\n",
    "        return wordnet.ADJ  #and then it can be mapped to the wordnet database\n",
    "                              #to get the lemma\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"Lemmatizes tokens with POS tagging\"\"\"\n",
    "    tagged_tokens = pos_tag(tokens)  \n",
    "    return [lemmatizer.lemmatize(word, pos) for word, tag in tagged_tokens\n",
    "            if (pos := get_wordnet_pos(tag))]\n",
    "\n",
    "data[\"lemmatized\"] = data[\"filtered_tokens\"].apply(lemmatize_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[appointment, dentist, last, reminder, email, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[site, several, month, release, beta, form, ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[everyone, recently, change, overall, enough, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[body, dysmorphia, disorder, social, anixety, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[point, content, never, process, stop, speak, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507604</th>\n",
       "      <td>[prepared, someday, gonna, matter, back, fact,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507605</th>\n",
       "      <td>[grow, external, validation, actual, happy, me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507606</th>\n",
       "      <td>[convenient, reason, unhappy, factor, external...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507607</th>\n",
       "      <td>[trust, old, superpower, age, awkward, everyon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507608</th>\n",
       "      <td>[save, marriage, reason, god, sex, marriage, i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>507609 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               lemmatized\n",
       "0       [appointment, dentist, last, reminder, email, ...\n",
       "1       [site, several, month, release, beta, form, ra...\n",
       "2       [everyone, recently, change, overall, enough, ...\n",
       "3       [body, dysmorphia, disorder, social, anixety, ...\n",
       "4       [point, content, never, process, stop, speak, ...\n",
       "...                                                   ...\n",
       "507604  [prepared, someday, gonna, matter, back, fact,...\n",
       "507605  [grow, external, validation, actual, happy, me...\n",
       "507606  [convenient, reason, unhappy, factor, external...\n",
       "507607  [trust, old, superpower, age, awkward, everyon...\n",
       "507608  [save, marriage, reason, god, sex, marriage, i...\n",
       "\n",
       "[507609 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[[\"lemmatized\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train bigram model on the lemmatized column\n",
    "bigram = Phrases(data[\"lemmatized\"], min_count=20)\n",
    "bigram_phraser = Phraser(bigram)  # Optimizes since it is a lighter-weight\n",
    "                                   #version of Phrases\n",
    "\n",
    "def add_bigrams(tokens):\n",
    "    \"\"\"Adds bigrams to a list of tokens if they appear frequently enough.\"\"\"\n",
    "    bigram_tokens = bigram_phraser[tokens]\n",
    "    return tokens + [token for token in bigram_tokens if '_' in token]\n",
    "\n",
    "data[\"ngrams\"] = data[\"lemmatized\"].apply(add_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering of too rare of too common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert df column to a list of lists for later steps\n",
    "docs = data[\"ngrams\"].tolist() \n",
    "\n",
    "# Create a dictionary from the tokenized documents\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that appear in fewer than 20 documents or \n",
    "#  more than 80% of documents, and keep the 1500 most frequent\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.8, keep_n=1500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bag of words representation\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1), (2, 1), (3, 2), (4, 2), (5, 1), (6, 1), (7, 1), (8, 1)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF IDF Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "temp = dictionary[0]  # Load the dictionary in memory \n",
    "tfidf = TfidfModel(corpus, id2word=dictionary.id2token,  dictionary=dictionary)\n",
    "tfidf_corpus = tfidf[corpus] #convert to weighted corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_threshold = 0.10 \n",
    "\n",
    "#Create new list to store filtered corpus by TF-IDF scores\n",
    "filtered_corpus = []\n",
    "\n",
    "for doc in corpus:\n",
    "    tfidf_doc = tfidf[doc]  # Get TF-IDF scores for the document\n",
    "    tfidf_dict = dict(tfidf_doc)  # Convert to dictionary {token_id: tfidf_score}\n",
    "\n",
    "    # Keep words from original BOW if their TF-IDF score is above the threshold\n",
    "    filtered_doc = [(token_id, count) for token_id, count in doc if tfidf_dict.get(token_id, 0) > tfidf_threshold]\n",
    "    \n",
    "    filtered_corpus.append(filtered_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "chunksize = 2000 #to speed up training\n",
    "passes = 10\n",
    "iterations = 100\n",
    "eval_every = None\n",
    "minimum_probability = 0.10  \n",
    "alpha = 0.2\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "temp = dictionary[0]  \n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=filtered_corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha=alpha,\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every,\n",
    "    random_state = 21,\n",
    "    minimum_probability=minimum_probability\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -2.4991.\n",
      "[([(0.03218448, 'even'),\n",
      "   (0.03017872, 'bad'),\n",
      "   (0.028814925, 'hard'),\n",
      "   (0.02834778, 'well'),\n",
      "   (0.019666387, 'everything'),\n",
      "   (0.019423438, 'back'),\n",
      "   (0.015077725, 'change'),\n",
      "   (0.014739713, 'best'),\n",
      "   (0.012448599, 'try'),\n",
      "   (0.012404177, 'good'),\n",
      "   (0.012116911, 'stop'),\n",
      "   (0.010669303, 'sometimes'),\n",
      "   (0.010175822, 'first'),\n",
      "   (0.009396984, 'need'),\n",
      "   (0.008899387, 'easy'),\n",
      "   (0.00876957, 'little'),\n",
      "   (0.008472593, 'matter'),\n",
      "   (0.008214693, 'actually'),\n",
      "   (0.008124072, 'shit'),\n",
      "   (0.008095185, 'time')],\n",
      "  -2.0278150915057602),\n",
      " ([(0.044777915, 'goal'),\n",
      "   (0.032973014, 'book'),\n",
      "   (0.0299162, 'step'),\n",
      "   (0.027918285, 'small'),\n",
      "   (0.027774546, 'start'),\n",
      "   (0.0197748, 'habit'),\n",
      "   (0.016458094, 'great'),\n",
      "   (0.015050282, 'term'),\n",
      "   (0.014921134, 'first'),\n",
      "   (0.013342873, 'read'),\n",
      "   (0.012248663, 'change'),\n",
      "   (0.011552645, 'progress'),\n",
      "   (0.010647659, 'write'),\n",
      "   (0.010219643, 'long'),\n",
      "   (0.010178938, 'plan'),\n",
      "   (0.009699708, 'list'),\n",
      "   (0.009550861, 'journey'),\n",
      "   (0.008812334, 'big'),\n",
      "   (0.008518113, 'easy'),\n",
      "   (0.008079569, 'new')],\n",
      "  -2.1981226921395938),\n",
      " ([(0.057714038, 'relationship'),\n",
      "   (0.046845004, 'never'),\n",
      "   (0.04653679, 'year'),\n",
      "   (0.03914155, 'always'),\n",
      "   (0.028789654, 'old'),\n",
      "   (0.025870128, 'young'),\n",
      "   (0.024959339, 'age'),\n",
      "   (0.02427139, 'high'),\n",
      "   (0.021790504, 'parent'),\n",
      "   (0.020335095, 'kid'),\n",
      "   (0.01708062, 'family'),\n",
      "   (0.01453588, 'child'),\n",
      "   (0.013964644, 'partner'),\n",
      "   (0.010975262, 'happy'),\n",
      "   (0.010974593, 'school'),\n",
      "   (0.010753122, 'love'),\n",
      "   (0.009382845, 'mom'),\n",
      "   (0.009159792, 'ever'),\n",
      "   (0.008997135, 'adult'),\n",
      "   (0.008345795, 'back')],\n",
      "  -2.2901651953259576),\n",
      " ([(0.020816263, 'world'),\n",
      "   (0.01942407, 'others'),\n",
      "   (0.012330562, 'positive'),\n",
      "   (0.012083904, 'value'),\n",
      "   (0.011939221, 'negative'),\n",
      "   (0.010630131, 'emotion'),\n",
      "   (0.009987715, 'human'),\n",
      "   (0.009471072, 'control'),\n",
      "   (0.00901086, 'experience'),\n",
      "   (0.008893237, 'mind'),\n",
      "   (0.00823197, 'change'),\n",
      "   (0.008230879, 'feeling'),\n",
      "   (0.00783251, 'happiness'),\n",
      "   (0.007332714, 'action'),\n",
      "   (0.0072781327, 'decision'),\n",
      "   (0.0071097123, 'moment'),\n",
      "   (0.006735952, 'purpose'),\n",
      "   (0.006354982, 'love'),\n",
      "   (0.0063229906, 'true'),\n",
      "   (0.006130054, 'happy')],\n",
      "  -2.3559010182403455),\n",
      " ([(0.041209962, 'friend'),\n",
      "   (0.021090178, 'everyone'),\n",
      "   (0.020773323, 'talk'),\n",
      "   (0.013503403, 'anyone'),\n",
      "   (0.01243668, 'different'),\n",
      "   (0.012418104, 'ask'),\n",
      "   (0.011633654, 'advice'),\n",
      "   (0.011178095, 'question'),\n",
      "   (0.010899543, 'else'),\n",
      "   (0.010843927, 'alone'),\n",
      "   (0.0100592645, 'comment'),\n",
      "   (0.009911741, 'conversation'),\n",
      "   (0.009688914, 'others'),\n",
      "   (0.009177486, 'kind'),\n",
      "   (0.007832414, 'actually'),\n",
      "   (0.0074568144, 'wrong'),\n",
      "   (0.0072261686, 'sure'),\n",
      "   (0.006995527, 'situation'),\n",
      "   (0.00688939, 'bro'),\n",
      "   (0.006839811, 'sometimes')],\n",
      "  -2.4017603001894754),\n",
      " ([(0.053261943, 'job'),\n",
      "   (0.040748134, 'money'),\n",
      "   (0.025635155, 'game'),\n",
      "   (0.01885919, 'home'),\n",
      "   (0.016571373, 'college'),\n",
      "   (0.015410321, 'school'),\n",
      "   (0.014305019, 'career'),\n",
      "   (0.0118976, 'business'),\n",
      "   (0.01183087, 'house'),\n",
      "   (0.010805246, 'car'),\n",
      "   (0.010490613, 'country'),\n",
      "   (0.009210706, 'pay'),\n",
      "   (0.008379271, 'video'),\n",
      "   (0.007952687, 'degree'),\n",
      "   (0.00778218, 'new'),\n",
      "   (0.007530474, 'place'),\n",
      "   (0.0073724315, 'live'),\n",
      "   (0.006997825, 'class'),\n",
      "   (0.006726316, 'video_game'),\n",
      "   (0.006603055, 'free')],\n",
      "  -2.4862698270930257),\n",
      " ([(0.027512452, 'body'),\n",
      "   (0.0255999, 'gym'),\n",
      "   (0.019448005, 'weight'),\n",
      "   (0.017404543, 'food'),\n",
      "   (0.017106038, 'sleep'),\n",
      "   (0.016465036, 'exercise'),\n",
      "   (0.015874173, 'hour'),\n",
      "   (0.015582385, 'day'),\n",
      "   (0.015556824, 'eat'),\n",
      "   (0.014922134, 'healthy'),\n",
      "   (0.013162954, 'minute'),\n",
      "   (0.011953193, 'routine'),\n",
      "   (0.011687698, 'brain'),\n",
      "   (0.01101825, 'energy'),\n",
      "   (0.010521166, 'water'),\n",
      "   (0.010246368, 'diet'),\n",
      "   (0.008660164, 'muscle'),\n",
      "   (0.0085053975, 'walk'),\n",
      "   (0.008404627, 'clean'),\n",
      "   (0.008112047, 'discipline')],\n",
      "  -2.6039377700380344),\n",
      " ([(0.073008105, 'woman'),\n",
      "   (0.03451183, 'men'),\n",
      "   (0.032567278, 'girl'),\n",
      "   (0.028672766, 'guy'),\n",
      "   (0.0265991, 'look'),\n",
      "   (0.025903529, 'sex'),\n",
      "   (0.023459964, 'man'),\n",
      "   (0.01922229, 'porn'),\n",
      "   (0.019190487, 'confidence'),\n",
      "   (0.016134592, 'attractive'),\n",
      "   (0.014255914, 'date'),\n",
      "   (0.013456454, 'ugly'),\n",
      "   (0.010097323, 'confident'),\n",
      "   (0.008512549, 'personality'),\n",
      "   (0.00804669, 'hair'),\n",
      "   (0.007984819, 'partner'),\n",
      "   (0.007927153, 'sexual'),\n",
      "   (0.0071643186, 'body'),\n",
      "   (0.00714252, 'face'),\n",
      "   (0.006787608, 'beautiful')],\n",
      "  -2.8064952248688986),\n",
      " ([(0.06703156, 'now'),\n",
      "   (0.06030377, 'help'),\n",
      "   (0.059212472, 'right'),\n",
      "   (0.03950825, 'issue'),\n",
      "   (0.036261093, 'health'),\n",
      "   (0.033020083, 'mental'),\n",
      "   (0.031968627, 'therapy'),\n",
      "   (0.030549455, 'problem'),\n",
      "   (0.024698112, 'anxiety'),\n",
      "   (0.023928046, 'therapist'),\n",
      "   (0.022803271, 'right_now'),\n",
      "   (0.018876549, 'depression'),\n",
      "   (0.0188403, 'addiction'),\n",
      "   (0.017660102, 'mental_health'),\n",
      "   (0.0125984335, 'professional'),\n",
      "   (0.011707484, 'doctor'),\n",
      "   (0.008155883, 'physical'),\n",
      "   (0.00793121, 'drug'),\n",
      "   (0.0075218733, 'adhd'),\n",
      "   (0.0067609046, 'support')],\n",
      "  -2.870544576837252),\n",
      " ([(0.06405756, 'social'),\n",
      "   (0.037091885, 'learn'),\n",
      "   (0.03465323, 'hobby'),\n",
      "   (0.030858452, 'skill'),\n",
      "   (0.030181734, 'medium'),\n",
      "   (0.029290944, 'new'),\n",
      "   (0.02519944, 'social_medium'),\n",
      "   (0.019952567, 'interest'),\n",
      "   (0.014613883, 'group'),\n",
      "   (0.014605837, 'music'),\n",
      "   (0.01339758, 'youtube'),\n",
      "   (0.013096964, 'phone'),\n",
      "   (0.012760293, 'watch'),\n",
      "   (0.0127555095, 'activity'),\n",
      "   (0.012395325, 'video'),\n",
      "   (0.011978599, 'art'),\n",
      "   (0.011785512, 'sport'),\n",
      "   (0.011274166, 'movie'),\n",
      "   (0.011152795, 'content'),\n",
      "   (0.00979811, 'apps')],\n",
      "  -2.950363437577519)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.500950930647863"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = CoherenceModel(model=model, corpus=corpus, texts=docs, coherence='c_v', dictionary=dictionary)\n",
    "cm.get_coherence()  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
